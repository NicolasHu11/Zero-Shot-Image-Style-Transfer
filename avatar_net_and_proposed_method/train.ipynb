{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from style_decorator import StyleDecorator\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvatarNet(nn.Module):\n",
    "    def __init__(self, layers=[1, 6, 11, 20]):\n",
    "        super(AvatarNet, self).__init__()\n",
    "        self.encoder = Encoder(layers)\n",
    "        self.decoder = Decoder(layers)\n",
    "        self.adain = AdaIN()\n",
    "        self.decorator = StyleDecorator()\n",
    "\n",
    "    def forward(self, content, styles, style_strength=1.0, patch_size=3, patch_stride=1, interpolation_weights=None):\n",
    "        if interpolation_weights is None:\n",
    "            interpolation_weights = [1/len(styles)] * len(styles)\n",
    "        content_feature = self.encoder(content)\n",
    "        style_features = []\n",
    "        for style in styles:\n",
    "            style_features.append(self.encoder(style))\n",
    "        transformed_feature = []\n",
    "        for style_feature, interpolation_weight in zip(style_features, interpolation_weights):\n",
    "            transformed_feature.append(self.decorator(content_feature[-1], style_feature[-1], style_strength, patch_size, patch_stride) * interpolation_weight)\n",
    "        transformed_feature = sum(transformed_feature)\n",
    "        style_features = [style_feature[:-1][::-1] for style_feature in style_features]\n",
    "        stylized_image = self.decoder(transformed_feature, style_features, interpolation_weights)\n",
    "        return stylized_image\n",
    "\n",
    "class AdaIN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdaIN, self).__init__()\n",
    "    \n",
    "    def forward(self, content, style, style_strength=1.0, eps=1e-5):\n",
    "        b, c, h, w = content.size()\n",
    "        content_std, content_mean = torch.std_mean(content.view(b, c, -1), dim=2, keepdim=True)\n",
    "        style_std, style_mean = torch.std_mean(style.view(b, c, -1), dim=2, keepdim=True)\n",
    "        normalized_content = (content.view(b, c, -1) - content_mean)/(content_std+eps)\n",
    "        stylized_content = (normalized_content * style_std) + style_mean\n",
    "        output = (1-style_strength)*content + style_strength*stylized_content.view(b, c, h, w)\n",
    "        return output\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,  layers=[1, 6, 11, 20]):        \n",
    "        super(Encoder, self).__init__()\n",
    "        vgg = torchvision.models.vgg19(pretrained=True).features\n",
    "        self.encoder = nn.ModuleList()\n",
    "        temp_seq = nn.Sequential()\n",
    "        for i in range(max(layers)+1):\n",
    "            temp_seq.add_module(str(i), vgg[i])\n",
    "            if i in layers:\n",
    "                self.encoder.append(temp_seq)\n",
    "                temp_seq = nn.Sequential()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "            features.append(x)\n",
    "        return features\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layers=[1, 6, 11, 20], transformers=[AdaIN(), AdaIN(), AdaIN(), None]):\n",
    "        super(Decoder, self).__init__()\n",
    "        vgg = torchvision.models.vgg19(pretrained=False).features\n",
    "        self.transformers = transformers\n",
    "        self.decoder = nn.ModuleList()\n",
    "        temp_seq  = nn.Sequential()\n",
    "        count = 0\n",
    "        for i in range(max(layers)-1, -1, -1):\n",
    "            if isinstance(vgg[i], nn.Conv2d):\n",
    "                out_channels = vgg[i].in_channels\n",
    "                in_channels = vgg[i].out_channels\n",
    "                kernel_size = vgg[i].kernel_size\n",
    "                temp_seq.add_module(str(count), nn.ReflectionPad2d(padding=(1,1,1,1)))\n",
    "                count += 1\n",
    "                temp_seq.add_module(str(count), nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size))\n",
    "                count += 1\n",
    "                temp_seq.add_module(str(count), nn.ReLU())\n",
    "                count += 1\n",
    "            elif isinstance(vgg[i], nn.MaxPool2d):\n",
    "                temp_seq.add_module(str(count), nn.Upsample(scale_factor=2))\n",
    "                count += 1\n",
    "            if i in layers:\n",
    "                self.decoder.append(temp_seq)\n",
    "                temp_seq  = nn.Sequential()\n",
    "        self.decoder.append(temp_seq[:-1])    \n",
    "        \n",
    "    def forward(self, x, styles, interpolation_weights=None):\n",
    "        if interpolation_weights is None:\n",
    "            interpolation_weights = [1/len(styles)] * len(styles)\n",
    "        y = x\n",
    "        for i, layer in enumerate(self.decoder):\n",
    "            y = layer(y)\n",
    "            if self.transformers[i]:\n",
    "                transformed_feature = []\n",
    "                for style, interpolation_weight in zip(styles, interpolation_weights):\n",
    "                    transformed_feature.append(self.transformers[i](y, style[i]) * interpolation_weight)\n",
    "                y = sum(transformed_feature)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lastest_arverage_value(values, length=100):\n",
    "    if len(values) < length:\n",
    "        length = len(values)\n",
    "    return sum(values[-length:])/length\n",
    "\n",
    "class ImageFolder(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_path, imsize=None, cropsize=None, cencrop=False):\n",
    "        super(ImageFolder, self).__init__()\n",
    "\n",
    "        self.file_names = sorted(os.listdir(root_path))\n",
    "        self.root_path = root_path\n",
    "        self.transform = _transformer(imsize, cropsize, cencrop)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(os.path.join(self.root_path + self.file_names[index])).convert(\"RGB\")\n",
    "        return self.transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imsave(tensor, path):\n",
    "    denormalize = _normalizer(denormalize=True)\n",
    "    if tensor.is_cuda:\n",
    "        tensor = tensor.cpu()\n",
    "    tensor = torchvision.utils.make_grid(tensor)\n",
    "    torchvision.utils.save_image(denormalize(tensor).clamp_(0.0, 1.0), path)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layers=[1, 6, 11, 20], transformers=[AdaIN(), AdaIN(), AdaIN(), None]):\n",
    "        super(Decoder, self).__init__()\n",
    "        vgg = torchvision.models.vgg19(pretrained=False).features\n",
    "        self.transformers = transformers\n",
    "        self.decoder = nn.ModuleList()\n",
    "        temp_seq  = nn.Sequential()\n",
    "        count = 0\n",
    "        for i in range(max(layers)-1, -1, -1):\n",
    "            if isinstance(vgg[i], nn.Conv2d):\n",
    "                out_channels = vgg[i].in_channels\n",
    "                in_channels = vgg[i].out_channels\n",
    "                kernel_size = vgg[i].kernel_size\n",
    "                temp_seq.add_module(str(count), nn.ReflectionPad2d(padding=(1,1,1,1)))\n",
    "                count += 1\n",
    "                temp_seq.add_module(str(count), nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size))\n",
    "                count += 1\n",
    "                temp_seq.add_module(str(count), nn.ReLU())\n",
    "                count += 1\n",
    "            elif isinstance(vgg[i], nn.MaxPool2d):\n",
    "                temp_seq.add_module(str(count), nn.Upsample(scale_factor=2))\n",
    "                count += 1\n",
    "            if i in layers:\n",
    "                self.decoder.append(temp_seq)\n",
    "                temp_seq  = nn.Sequential()\n",
    "        self.decoder.append(temp_seq[:-1])    \n",
    "        \n",
    "    def forward(self, x, styles, interpolation_weights=None):\n",
    "        if interpolation_weights is None:\n",
    "            interpolation_weights = [1/len(styles)] * len(styles)\n",
    "        y = x\n",
    "        for i, layer in enumerate(self.decoder):\n",
    "            y = layer(y)\n",
    "            if self.transformers[i]:\n",
    "                transformed_feature = []\n",
    "                for style, interpolation_weight in zip(styles, interpolation_weights):\n",
    "                    transformed_feature.append(self.transformers[i](y, style[i]) * interpolation_weight)\n",
    "                y = sum(transformed_feature)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_train(content_dir):\n",
    "    # set device\n",
    "    device = torch.device('cuda')\n",
    "    # get network\n",
    "    network = AvatarNet([1, 6, 11, 20]).to(device)\n",
    "    # get data set\n",
    "    data_set = ImageFolder(content_dir, 176, 176, True)\n",
    "    # get loss calculator\n",
    "    loss_network = Encoder([1, 6, 11, 20]).to(device)\n",
    "    mse_loss = torch.nn.MSELoss(reduction='mean').to(device)\n",
    "    loss_seq = {'total':[], 'image':[], 'feature':[], 'tv':[]}\n",
    "    # get optimizer\n",
    "    for param in network.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    optimizer = torch.optim.Adam(network.decoder.parameters(), lr=1e-3)\n",
    "    # training\n",
    "    for iteration in range(20000):\n",
    "        data_loader = torch.utils.data.DataLoader(data_set, batch_size=16, shuffle=True)\n",
    "        input_image = next(iter(data_loader)).to(device)\n",
    "        if (iteration + 1) % 100 == 0:\n",
    "            print(iteration+1, \n",
    "                20000)\n",
    "            torch.save({'iteration': iteration+1,\n",
    "                'state_dict': network.state_dict()\n",
    "               },\n",
    "                './'+'model.pth')\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalizer(denormalize=False):\n",
    "    MEAN = [0.485, 0.456, 0.406]\n",
    "    STD = [0.229, 0.224, 0.225]    \n",
    "    if denormalize:\n",
    "        MEAN = [-mean/std for mean, std in zip(MEAN, STD)]\n",
    "        STD = [1/std for std in STD]\n",
    "    return transforms.Normalize(mean=MEAN, std=STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transformer(imsize=None, cropsize=None, cencrop=False):\n",
    "    normalize = _normalizer()\n",
    "    transformer = []\n",
    "    if imsize:\n",
    "        transformer.append(transforms.Resize(imsize))\n",
    "    if cropsize:\n",
    "        if cencrop:\n",
    "            transformer.append(transforms.CenterCrop(cropsize))\n",
    "        else:\n",
    "            transformer.append(transforms.RandomCrop(cropsize))\n",
    "\n",
    "    transformer.append(transforms.ToTensor())\n",
    "    transformer.append(normalize)\n",
    "    return transforms.Compose(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 20000\n",
      "200 20000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-46d0aa79af0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnetwork_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./image/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-80a19c0f869b>\u001b[0m in \u001b[0;36mnetwork_train\u001b[1;34m(content_dir)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mdata_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0minput_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "network_train('./image/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
